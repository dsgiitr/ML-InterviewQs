# Put your questions here:

- title: Gradient Descent dependence on order of training data
  type: mcq
  text: Which of the following optimization algorithms does not depend on the order of training data?
  opt:
    - Batch Gradient Descent
    - Mini-batch Gradient Descent
    - Stochastic Gradient Descent
  ans: 
    - Batch Gradient Descent
  q_img: NULL
  sol:
    A full batch gradient descent goes through the entire dataset in every iteration. Also, the operations we perform are commutative and hence the order of the training examples does not matter.
  sol_img: NULL
  tags: 
    - gradient-descent
  diff: easy
  ref: NULL

- title: Need for feature scaling
  type: mcq
  text: Which of these methods does not require you to do feature scaling on the input data?
  opt:
    - Principal Component Analysis (PCA)
    - Support Vector Machines (SVM)
    - Decision Trees
    - K Means Clustering
  ans: 
    - Decision Trees 
  q_img: NULL
  sol:
    Decision trees don't require feature scaling as they use a rule-based approach instead of calculating distances.
  sol_img: NULL
  tags: 
    - svm
    - trees
  diff: easy
  ref: NULL

- title: Random Forest vs Boosting
  type: mcq
  text: Which statement is false when comparing the random forest algorithm with gradient boosting methods?
  opt:
    - Both are ensemble methods.
    - Both use a learning rate to minimize the loss function.
    - Gradient boosting methods typically use shallower trees than random forest.
    - With random forest, each tree is trained independently from the others.
  ans: 
    - Both use a learning rate to minimize the loss function.
  q_img: NULL
  sol:
    - Both random forest and boosting are ensemble methods. However, random forest relies on the weak law of large numbers for its accuracy, that is, it trains a lot of trees independently, and then selects the model of all the predictions made by the individual decision trees (majority voting) and then returns the result as its final prediction. However, in gradient boosting methods, the trees are not built independently but instead they are built in a sequential manner where each tree effectively learns the mistake from the ones that come before it. Now, since random forest trains each tree independently, it can afford to have deep trees since deep trees have low bias and high variance, and the high variance gets reduced due to model averaging. On the other hand, in gradient boosting methods, if a single tree is deep, it might overfit and get stuck in a local minima very soon. So, it's better to use shallower trees since each of them will have low variance due to low complexity and the large number of trees will also reduce bias. Now, coming to the last option, gradient boosting methods use gradients to optimize the loss where a learning rate is required to control the step size, whereas random forest uses the bagging method which does not require a learning rate.
  sol_img: NULL
  tags: 
    - gradient-descent
    - trees
    - ensemble-learning
    - supervised
    - boosting
    - random-forest
  diff: medium
  ref: Microsoft

- title: Feature selection methods in Machine Learning
  type: mcq
  text: Which of the following algorithms are not suitable for feature selection before training a machine learning model?
  opt:
    - Chi-Square Test
    - Lasso Regression
    - ANOVA (Analysis of Variance)
    - Ridge Regression
  ans: 
    - Ridge Regression
  q_img: NULL
  sol:
    Both Chi-Square Test and ANOVA are statistical tests which select features on the basis of their correlation with the outcome variable. Lasso regression shrinks the coefficients of the irrelevant features to zero thus removing them altogether which results in a useful set of selected features. On the contrary Ridge regression shrinks the coefficient estimate towards zero, but not exactly zero which is why it is not suitable for feature selection.
  sol_img: NULL
  tags: 
    - regularization
    - dimensionality-reduction
  diff: easy
  ref: NULL