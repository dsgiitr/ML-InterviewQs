{"0": {
    "doc": "About us",
    "title": "About Us",
    "content": "Website GitHub Mail Facebook Twitter LinkedIn . ",
    "url": "/about#about-us",
    "relUrl": "/about#about-us"
  },"1": {
    "doc": "About us",
    "title": "About us",
    "content": " ",
    "url": "/about",
    "relUrl": "/about"
  },"2": {
    "doc": "New to Open Source?",
    "title": "New to Open Source?",
    "content": "If you are new to contributing to open source, this guide helps explain why, what, and how to get involved. Here are some resources to help you get started with open source contributions: . | Finding ways to contribute to open source on GitHub | Set up Git | GitHub flow | Collaborating with pull requests | . We thank you for investing your time in contributing to this project! . Create a new Issue . If you spot a problem in this repository, or need to request a new tag, or a new key, search if an issue already exists. If a related issue doesn’t exist, you can open a new issue using a relevant issue form. Solve an Issue . Scan through our existing issues to find one that interests you. You can narrow down the search using labels as filters. See Labels for more information. Style guide . We follow standard effective yaml guidelines. You can refer to this guide for more information. Keep these in mind while creating a pull request: . | Don’t forget to link PR to issue if you are solving one. | We may ask for changes to be made before a PR can be merged, either using suggested changes or pull request comments. | . ",
    "url": "/basics",
    "relUrl": "/basics"
  },"3": {
    "doc": "Contribute",
    "title": "Machine Learning Interview Questions",
    "content": "This repository aims to develop and compile a list of questions focussed on machine learning interviews for various data science roles, to help prepare candidates for the same. Each question and its solution is critically reviewed and is structured with metadata tags to help sort and identify questions based on topics, difficuty etc. ",
    "url": "/contribute#machine-learning-interview-questions",
    "relUrl": "/contribute#machine-learning-interview-questions"
  },"4": {
    "doc": "Contribute",
    "title": "Call for Contributions",
    "content": "We welcome community contributions, feel free to add your own question and solution to improve the question bank. Adding new questions isn’t the only way to contribute, small improvements or fixes in the questions/solutions existing already are always appreciated. You can also help review pull requests, triage issues and suggest more features. If you’re unsure where to start, have a look at our Contribution Guide for more details or reach out at dsg@iitr.ac.in. ",
    "url": "/contribute#call-for-contributions",
    "relUrl": "/contribute#call-for-contributions"
  },"5": {
    "doc": "Contribute",
    "title": "Contribute",
    "content": " ",
    "url": "/contribute",
    "relUrl": "/contribute"
  },"6": {
    "doc": "Deep Learning",
    "title": "Deep Learning",
    "content": " ",
    "url": "/dl",
    "relUrl": "/dl"
  },"7": {
    "doc": "Deep Learning",
    "title": "Difference between LSTMs and GRUs",
    "content": "Problem . Which of the following is not true with respect to LSTMs and GRUs? . Options . | Both LSTM and GRU cells have a gating mechanism. An LSTM cell has three gates, while a GRU cell has two. | . | LSTM has a memory cell state while GRU, similar to RNN does not. | . | The LSTM cells use tanh and sigmoid non-linearities in its update equations while the GRU cells use only tanh. | . | A GRU cell has a lower number of parameters than an LSTM cell. | . Answer . The LSTM cells use tanh and sigmoid non-linearities in its update equations while the GRU cells use only tanh. Solution . The update equation of the GRU cells also uses both tanh and sigmoid functions. ",
    "url": "/dl#difference-between-lstms-and-grus",
    "relUrl": "/dl#difference-between-lstms-and-grus"
  },"8": {
    "doc": "Deep Learning",
    "title": "Discussing dropout",
    "content": "Problem . Which of the following are true regarding dropout? . Options . | Dropout is similar to bagging training. | . | Dropout needs to be turned off during testing. | . | Both of the above. | . | Causes the model to overfit on the training data. | . Answer . Dropout needs to be turned off during testing. Solution . Dropout needs to be turned off during testing as we want the entire representational capacity of the model during testing. Dropout is similar to a bagging model but there are a few key differences. First in bagging the models are trained independently of each other whereas the dropout models share parameters with each other. Second, in bagging each individual model is trained to convergence while that is not the case in dropout (where each model is trained for 1 step). Further D is incorrect since dropout helps reduce overfitting. ",
    "url": "/dl#discussing-dropout",
    "relUrl": "/dl#discussing-dropout"
  },"9": {
    "doc": "Home",
    "title": "Machine Learning Interview Questions",
    "content": "To get started, click on one of the below categories. You can also use the search utility on the top to search through all the added questions. | Probability Statistics | Machine Learning | . | Deep Learning | Reinforcement Learning | . | Miscelleneous |   | . To contribute, see the contributions page . ",
    "url": "/#machine-learning-interview-questions",
    "relUrl": "/#machine-learning-interview-questions"
  },"10": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"11": {
    "doc": "Miscelleneous",
    "title": "Miscelleneous",
    "content": " ",
    "url": "/misc",
    "relUrl": "/misc"
  },"12": {
    "doc": "Machine Learning",
    "title": "Machine Learning",
    "content": " ",
    "url": "/ml",
    "relUrl": "/ml"
  },"13": {
    "doc": "Machine Learning",
    "title": "Gradient Descent dependence on order of training data",
    "content": "Problem . Which of the following optimization algorithms does not depend on the order of training data? . Options . | Batch Gradient Descent | . | Mini-batch Gradient Descent | . | Stochastic Gradient Descent | . Answer . Batch Gradient Descent . Solution . A full batch gradient descent goes through the entire dataset in every iteration. Also, the operations we perform are commutative and hence the order of the training examples does not matter. ",
    "url": "/ml#gradient-descent-dependence-on-order-of-training-data",
    "relUrl": "/ml#gradient-descent-dependence-on-order-of-training-data"
  },"14": {
    "doc": "Machine Learning",
    "title": "Need for feature scaling",
    "content": "Problem . Which of these methods does not require you to do feature scaling on the input data? . Options . | Principal Component Analysis (PCA) | . | Support Vector Machines (SVM) | . | Decision Trees | . | K Means Clustering | . Answer . Decision Trees . Solution . Decision trees don’t require feature scaling as they use a rule-based approach instead of calculating distances. ",
    "url": "/ml#need-for-feature-scaling",
    "relUrl": "/ml#need-for-feature-scaling"
  },"15": {
    "doc": "Machine Learning",
    "title": "Random Forest vs Boosting",
    "content": "Problem . Which statement is false when comparing the random forest algorithm with gradient boosting methods? . Options . | Both are ensemble methods. | . | Both use a learning rate to minimize the loss function. | . | Gradient boosting methods typically use shallower trees than random forest. | . | With random forest, each tree is trained independently from the others. | . Answer . Both use a learning rate to minimize the loss function. Solution . Both random forest and boosting are ensemble methods. However, random forest relies on the weak law of large numbers for its accuracy, that is, it trains a lot of trees independently, and then selects the model of all the predictions made by the individual decision trees (majority voting) and then returns the result as its final prediction. However, in gradient boosting methods, the trees are not built independently but instead they are built in a sequential manner where each tree effectively learns the mistake from the ones that come before it. Now, since random forest trains each tree independently, it can afford to have deep trees since deep trees have low bias and high variance, and the high variance gets reduced due to model averaging. On the other hand, in gradient boosting methods, if a single tree is deep, it might overfit and get stuck in a local minima very soon. So, it’s better to use shallower trees since each of them will have low variance due to low complexity and the large number of trees will also reduce bias. Now, coming to the last option, gradient boosting methods use gradients to optimize the loss where a learning rate is required to control the step size, whereas random forest uses the bagging method which does not require a learning rate. ",
    "url": "/ml#random-forest-vs-boosting",
    "relUrl": "/ml#random-forest-vs-boosting"
  },"16": {
    "doc": "Machine Learning",
    "title": "Feature selection methods in Machine Learning",
    "content": "Problem . Which of the following algorithms are not suitable for feature selection before training a machine learning model? . Options . | Chi-Square Test | . | Lasso Regression | . | ANOVA (Analysis of Variance) | . | Ridge Regression | . Answer . Ridge Regression . Solution . Both Chi-Square Test and ANOVA are statistical tests which select features on the basis of their correlation with the outcome variable. Lasso regression shrinks the coefficients of the irrelevant features to zero thus removing them altogether which results in a useful set of selected features. On the contrary Ridge regression shrinks the coefficient estimate towards zero, but not exactly zero which is why it is not suitable for feature selection. ",
    "url": "/ml#feature-selection-methods-in-machine-learning",
    "relUrl": "/ml#feature-selection-methods-in-machine-learning"
  },"17": {
    "doc": "Probability and Statistics",
    "title": "Probability and Statistics",
    "content": " ",
    "url": "/probstats",
    "relUrl": "/probstats"
  },"18": {
    "doc": "Add Questions",
    "title": "Add Questions",
    "content": "All the questions are stored in .yaml files within the _data folder. For more information about .yaml files, please visit: . | The official YAML website | YAML Syntax | . Segregation . Questions are divided into five broad categories: . | Machine Learning | Deep Learning | Probability and Statistics | Reinforcement Learning | Miscellaneous | . To request the addition of a new category you can create an issue. YAML list of acceptable values for each key . Each question is stored in a key-value format. There are a fixed set of keys, some of which must neccessarily have non-NULL values. Here is the list of keys alongwith the list of values they can accept: . | Key | Description | Accepted Values | . | title | The title of the question (for later review) | any string | . | type | This specifies the type of question (e.g., MCQs, descriptive, etc.). | mcq, descriptive, num | . | text | The main body of the question. | any string | . | opt | The list of options in case the question is a multiple choice question. | array, NULL | . | ans | The final answer. | any string | . | q_img | The path of the image file(s) that you may want to display with the question. | array, NULL | . | sol | Further explanation that you may include for how you arrived at the answer. | any string, NULL | . | sol_img | The path of the image file(s) that you may want to display with the solution. | array, NULL | . | tags | You can assign tags to your question based on the domains it requires knowledge of. | List of Tags | . | diff | The difficulty level of the question | easy, medium, hard | . | ref | References (Where did you encounter the question) | any string | . To request the addition of a new key you can create an issue. ",
    "url": "/questions",
    "relUrl": "/questions"
  },"19": {
    "doc": "Reinforcement Learning",
    "title": "Reinforcement Learning",
    "content": " ",
    "url": "/rl",
    "relUrl": "/rl"
  },"20": {
    "doc": "Question Tags",
    "title": "Question Tags",
    "content": "What are Tags? . Each question can be assigned various tags based on the domains it requires knowledge of. For example, consider the question: . Q. Using which layer can you make your CNN invariant to the size of the input? A. Global Average Pooling. Similar to max pooling layers, GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. However, GAP layers perform a more extreme type of dimensionality reduction, where a tensor with dimensions h×w×d is reduced in size to have dimensions 1×1×d. GAP layers reduce each h×w feature map to a single number by simply taking the average of all hw values, making the network invariant to the size of the input. This question may be tagged with the tags cnn, pooling. Why use Tags? . Assigning tags to each question allows for better and finer segregation of questions, and is an important aspect of the application we’re building. List of Tags . Currently, there is a standard list of tags to be used, in order to avoid duplicate tags: . | gradient-descent | regression | softmax | regularization | . | multicollinearity | svm | trees | ensemble-learning | . | validation | nlp | cv | eda | . | clustering | unsupervised | supervised | bayes-theorem | . | naive-bayes | hypothesis-testing | statistics | knn | . | dimensionality-reduction | metrics | cnn | pooling | . | ann | rnn | backprop | random-forest | . | boosting |   |   |   | . To request the addition of a new tag, create an issue. ",
    "url": "/tags",
    "relUrl": "/tags"
  }
}
